target Python {
  coordination: decentralized
}

preamble {=
  import time
  import numpy as np
  import pyarrow.plasma as plasma
  session = plasma.connect("/tmp/plasma")
  import pickle

  def serialization(session, data):
      buffer = []
      pickled_data = pickle.dumps(data, protocol=5, buffer_callback=buffer.append)
      data_object_id = pickled_data

      # Store memoryview in Plasma
      mem_view = buffer[0].raw()
      buffer_id = plasma.ObjectID.from_random()
      data_size = len(mem_view)

      # Create an object in Plasma and copy the memoryview data into it
      plasma_object = session.create(buffer_id, data_size)
      plasma_view = memoryview(plasma_object).cast('B')
      plasma_view[:] = mem_view[:]

      session.seal(buffer_id)

      # Return
      return [data_object_id, buffer_id]

  def deserialization(session, data_object_id, buffer_id):
      #memory getting
      buffer = session.get_buffers([buffer_id])
      pickled_data = data_object_id
      data = pickle.loads(pickled_data, buffers=buffer)
      return data
=}

reactor clientReactor(STP_offset = forever){
  input global_parameters
  output updated_parameters

  reaction(global_parameters) -> updated_parameters {=
    data_pair = global_parameters.value
    val = deserialization(session, data_pair[0], data_pair[1])
    time.sleep(0.5)
    new_parameter = val.copy()
    id  = serialization(session, new_parameter)
    updated_parameters.set(id)
  =}
}

reactor serverReactor{
  output global_parameters
  input[4] updated_parameters
  state round_num
  state start_time
  state prev_time
  state object_size = 50

  reaction(startup) -> global_parameters {=
    self.round_num = 0
    self.results = [0] * 4
    n_rows = 62500 * self.object_size  # 62500 rows is 1MB
    val = np.random.random((n_rows, 2))  # 2 columns for x and y
    id  = serialization(session, val)
    self.prev_time = self.start_time = time.time()
    global_parameters.set(id)
  =}

  reaction(updated_parameters) -> global_parameters {=
    # Retrieve value from each client
    for i in range(4):
        data_pair = updated_parameters[i].value
        self.results[i] = deserialization(session, data_pair[0], data_pair[1])

    cur_time = time.time()
    self.round_num += 1
    print("Episode: "+str(self.round_num))
    print(f"Time taken: {cur_time - self.start_time:.5f} seconds")
    print(f"Overhead: {cur_time - self.prev_time - 0.5:.5f} seconds\n")
    if self.round_num == 10:
        if not os.path.exists("./logs/benchmarks"):
            os.makedirs("./logs/benchmarks")
        if os.path.exists("./logs/benchmarks/gather.pkl"):    
            with open("./logs/benchmarks/gather.pkl", "rb") as f:
                results = pickle.load(f)
        else:
            results = {}
        if "hprm_decentralized" not in results.keys():
            results["hprm_decentralized"] = {} 
        results["hprm_decentralized"][str(self.object_size)] = (cur_time - self.start_time) / self.round_num - 0.5,
        with open("./logs/benchmarks/gather.pkl", "wb") as f:
            pickle.dump(results, f)
        print("Benchmark done.")
        request_stop()
        return
    self.prev_time = cur_time

    # Update the global parameters with the results from the first client for the next round
    id = serialization(session, self.results[0])
    global_parameters.set(id)
  =} STP(forever){==}
}

federated reactor {
  client = new[4] clientReactor()
  server = new serverReactor()
  (server.global_parameters)+ -> client.global_parameters 
  client.updated_parameters -> server.updated_parameters after 0
}
